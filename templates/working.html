<!DOCTYPE html>

{% extends "base.html" %}
{% load staticfiles %}
{% block title %}Working{% endblock %}

{% block extra_head %}
<link rel="stylesheet" href="{% static 'css/base.css' %}">
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
{% endblock %}


{% block body_block %}
  <div class="container">
    <div id="content">
    <h1>This model uses Hybrid learning approach viz. <br> Unsupervised learning using Self Organizing Maps and <br>Supervised learning using ANN </h1>
    <p class="pspace">Let’s have a close look at the example dataset.</p>
    <br>
    <hr>
    <p class="grey">dataset = pd.read_csv(‘Credit_card_data.csv’)</p>
    <img src="../static/images/1.png" alt="">
    <p class="pspace">As seen in fig 2, there are 14 attributes labelled A1-A14. The first column indicates the customer ID to identify a particular customer and the last column mentions the class.
      Class = 0 reflects that customer didn’t get credit card from bank and Class = 1 indicates that customer got his/her credit card application approved from the bank.
    </p>
    <h2>1) Unsupervised learning</h2>
    <p class="pspace">Apply a Self Organizing Map (SOM) on the dataset ignoring the class column. We leverage Class column later. Thus we feed 15 columns (CustomerID + 14 Attributes) to the SOM.</p>
    <img src="../static/images/2.png" alt="">
    <p class="pspace">Each customer’s attribute is fitted to one of the neuron (Winner neuron for that row) out of 15 x 15 nodes on the 2D map. Thus entire dataset is converged on the map with each neuron/node containing many customers having some correlations in their attributes.</p>
    <br>
    <hr>
    <p class="pspace">Once we have mapped each row of our dataset to each node on the 15x15 SOM map, we then move to find outliers. This means, we resort to find out as to which neuron/node appears strange from the rest of the lot. Once we find outlier nodes/ neurons, we can then find the customers grouped under them and they will be the most probable frauds.
      To find outliers, I utilize mean interneuron distance (MID). If MID for a node/ neuron is high, it indicates that the node/ neuron is an outlier and that it appears different from the rest of the nodes. The customers grouped in those nodes/ neurons are then our highest probable frauds.
    </p>
    <img src="../static/images/3.png" alt="">
    <p class="pspace">As seen in Fig above, the node circled in red is surely an outlier and the customers grouped in that neurons are our most probable frauds.</p>
    <br>
    <hr>
    <p class="pspace">Now lets extract the customer ids of the customers grouped in node (6,1). (Outlier node)</p>
    <img src="../static/images/4.png" alt="">
    <h1 style="color:red;">There they are ! These extracted 8 customers are our highest probable frauds. !</h1>

    <h2>2) Supervised learning</h2>
    <p class="pspace">Now, to further improve accuracy, we train an artifical neural network with 15 columns as <strong>features</strong> and the frauds detected by SOM as <strong>Target !</strong></p>
    
    </div>
  </div>

  {% endblock %}
